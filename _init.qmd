```{r settings}
#| include: false
library(glue)
library(ggplot2)
library(scales)
library(tidyr)
library(dplyr)
library(jsonlite)
library(readr)
library(stringr)
library(rmarkdown)
library(DT)
library(lubridate)
library(leaflet)
library(data.table)
library(furrr)
library(purrr)
library(rlang)

theme_set(theme_classic(base_size = 12))
colorBlindBlack8 <- c(
    "#464343", "#E69F00", "#56B4E9", "#009E73",
    "#F0E442", "#0072B2", "#D55E00", "#CC79A7",
    "#750a0f", "#669c8e", "#ffaabb", "#cbbb45"
)
options(ggplot2.discrete.colour = colorBlindBlack8)
options(ggplot2.discrete.fill = colorBlindBlack8)

```

```{r projectIds}
#| include: false
#| cache: true

#projectIds <- c("Demo" = "27173")

projectIds <- c("Mittelkärnten" = "218931", "Gesäuse" = "225285", "Industrieviertel" = "231426", "Krems" = "220961", "Marchegg" = "229928", "St.Pölten" = "228867", "Elsbeere" = "225365", "Neusiedlersee" = "224160", "Amstetten" = "228551", "Graz" = "220708", "Innsbruck" = "226061", "Wien" = "224108")

# Observation.org
locationIds <- c("Salzburg" = "606489", "Vorarlberg" = "606492")
```


```{r loadData}
#| include: false
#| cache: true

if (file.exists('data/checkList.rds')) {
  checkList <- readRDS('data/checkList.rds')
  birdsList <- readRDS('data/birdsList.rds')
  taxas <- readRDS('data/taxas.rds')
} else {
  # RedList
  checkList <- fread(
      "data/data_species_check_list.csv", sep = ",", dec=".", stringsAsFactors = FALSE, encoding = "UTF-8", colClasses = "character"
    ) |>
    rename(scientific_name = speciesname) |>
    distinct(scientific_name, .keep_all = TRUE) |>
    filter((annex_II_priority != "N" | annex_II != "N" | annex_IV != "N")) 
  # Birds RedList
  birdsList <- fread("data/birds.csv", sep = ";", dec=".", stringsAsFactors = FALSE, encoding = "UTF-8", colClasses = "character") |>
    rename_with(~"scientific_name", 1) |>
    select(scientific_name, `Annex I`) |>
    rename(AnnexI = `Annex I`) |>
    drop_na(AnnexI)
# German Common Names
  taxasDE <- fread(
      'data/VernacularNames-german.csv', sep = ",", dec=".", stringsAsFactors = FALSE, encoding = "UTF-8", colClasses = "character"
    )[, .(vernacularName = stringr::str_trunc(paste(vernacularName, collapse = ', '), 35)), by = id]

  # Full Taxon Tree Names
  taxas <- fread('data/taxa.csv', sep = ",", dec=".", stringsAsFactors = FALSE, encoding = "UTF-8", colClasses = "character")[
    taxasDE, on = "id"][, .(id, scientificName, vernacularName, taxonRank, kingdom, phylum, class, order, family, genus, specificEpithet, infraspecificEpithet)]

  rm(taxasDE)
  saveRDS(checkList, 'data/checkList.rds')
  saveRDS(taxas, 'data/taxas.rds')
  saveRDS(birdsList, 'data/birdsList.rds')
}
```

```{r loadFunctions}
#| include: false

fetchTxt <- function(url){
  return(base::url(url, headers = c(Accept = "application/json, text/*, */*", "User-Agent" = "inat-austria-city-challenge-2025", "Accept-Language" = "de-AT, en-US")))
}

# iNaturalist
loadINat <- function(projectId, maxLoops = 1000){
   tryCatch(
     exp = {
      checkIfFileExists <- file.exists(glue("data/inat_{projectId}.csv"))
      if(checkIfFileExists){
        message("File exists, loading from file")
        resultsDf <- fread(
          glue("data/inat_{projectId}.csv"), sep = ",", dec=".", stringsAsFactors = FALSE, encoding = "UTF-8", colClasses = "character"
        )
        return(resultsDf)
      }
      apiEndpoint <- "https://api.inaturalist.org/v2/observations"
      fields <- "(id:!t,uri:!t,quality_grade:!t,num_identification_disagreements:!t,time_observed_at:!t,location:!t,user:(name:!t,id:!t,login:!t),taxon:(id:!t),identifications:(user:(name:!t,login:!t)))"
      perPage <- 200
      staticQuery <- glue("project_id={projectId}&order=asc&order_by=id&per_page={perPage}&fields={fields}")
      
      aboveId <- 1
      obsList <- list()
      loopCounter <- 1
      while (aboveId > 0) {
        start_time <- Sys.time()

        obsUrl <- glue("{apiEndpoint}?{staticQuery}&id_above={aboveId}")
        message(obsUrl)
        obsFetch <- fetchTxt(obsUrl) |> jsonlite::fromJSON()
        totalResults <- obsFetch$total_results
        if(totalResults == 0){
          message("Empty results")
          break;
        }
        
        obsTempResults <- obsFetch$results |> unnest(c(taxon, user), names_sep = ".")
        aboveId <- max(obsTempResults$id)
        obsList[[loopCounter]] <- obsTempResults |>
          mutate(across(everything(), as.character)) |>
          mutate(
            # Dirty workaround for later parsing of list
            identifications = str_replace_all(identifications, "\'", "`"),
            identifications = str_replace_all(identifications, "\"", "'")
            )
        
        if(totalResults <= perPage){
          message("No more fetches needed")
          break;
        }
        
        message("Remaining Obs")
        message(totalResults - perPage)
        message("Loops to finish")
        message((totalResults - perPage) / perPage)
        message("Loops done")
        message(loopCounter)
        
        if(loopCounter >= maxLoops){
          warning("Breaking early because max loop limit")
          break;
        }
        
        loopCounter <- loopCounter + 1
        elapsed_time <- Sys.time() - start_time
        sleep_time <- max(0, 1 - as.numeric(elapsed_time, units = "secs"))
        Sys.sleep(sleep_time)
      }
      if(length(obsList) == 0){
        message("No results")
        return(NULL)
      }
      resultsDf <- bind_rows(obsList) |>
        left_join(taxas, by = c("taxon.id" = "id"))|> 
        mutate(
          user.name = ifelse(is.na(user.name) | user.name == '', user.login, user.name),
          project.id = projectId,
          project.name = names(projectId),
          project.type = "inat",
          uri_html = glue("<a href='{uri}'>{id}</a>"),
        ) |>
        distinct(uuid, .keep_all = TRUE) # catch any possible double entries due to our loop logic

      write_csv(resultsDf, paste0("data/inat_", projectId, ".csv"), quote = "all")
      return(resultsDf)
     },
    error = function(e){
      message('Caught an error!')
      print(e)
      return(NULL)
    },
    warning = function(w){
      message('Caught an warning!')
      print(w)
      return(NULL)
    },
    finally = {
      message('All done, quitting.')
    }
  )
}

# https://observation.org/api/v1/lookups
lookup <- jsonlite::fromJSON("data/lookup.json") |>
          map(~ bind_rows(.x)) |>
          imap(\(x, idx) x %>% rename_with(~paste0(idx, "_name", recycle0 = TRUE), .cols = "name"))



# Observation.org
loadObs <- function(locationId, maxLoops = 1000, date_after = "2025-04-25", date_before = "2025-04-28"){
   tryCatch(
     exp = {
      checkIfFileExists <- file.exists(glue("data/obs_{locationId}.csv"))
      if(checkIfFileExists){
        message("File exists, loading from file")
        resultsDf <- fread(
          glue("data/obs_{locationId}.csv"), sep = ",", dec=".", stringsAsFactors = FALSE, encoding = "UTF-8", colClasses = "character"
        )
        return(resultsDf)
      }
      apiEndpoint <- glue("https://observation.org/api/v1/locations/{locationId}/observations/")
      perPage <- 5000
      staticQuery <- glue("limit={perPage}&date_after={date_after}&date_before={date_before}")
      obsUrl <- glue("{apiEndpoint}?{staticQuery}")

      obsList <- list()
      loopCounter <- 1
      while (TRUE) {
        start_time <- Sys.time()

        message(obsUrl)
        obsFetch <- fetchTxt(obsUrl) |> jsonlite::fromJSON()
        totalResults <- obsFetch$count
        if(totalResults == 0){
          message("Empty results")
          break;
        }
        
        obsTempResults <- obsFetch$results
        
        aboveId = max(obsTempResults$id)
        obsList[[loopCounter]] <- obsTempResults
        
        if(totalResults <= perPage){
          message("No more fetches needed")
          break;
        }
        
        if(is.null(obsFetch$`next`)){
          message('All fetched')
          break;
        }
        obsUrl <- obsFetch$`next`
        
        message("Remaining Obs")
        message(totalResults - perPage)
        message("Loops to finish")
        message((totalResults - perPage) / perPage)
        message("Loops done")
        message(loopCounter)
        
        if(loopCounter >= maxLoops){
          warning("Breaking early because max loop limit")
          break;
        }
        
        loopCounter <- loopCounter + 1
        elapsed_time <- Sys.time() - start_time
        #sleep_time <- max(0, 1 - as.numeric(elapsed_time, units = "secs"))
        #Sys.sleep(sleep_time)
      }
      
      if(length(obsList) == 0){
        message("No results")
        return(NULL)
      }
      
      resultsDf <- bind_rows(obsList)  |>
            unnest_wider(species_detail, names_sep = "_") |>
            unnest_wider(point, names_sep = "_") |>
            drop_na(point_type, point_coordinates) |>

            left_join(lookup$species_type, by = c("species_detail_type" = "id")) |>
            left_join(lookup$rarity, by = c("rarity" = "id")) |>
            left_join(lookup$validation_status, by = c("validation_status" = "id")) |>
            left_join(lookup$species_status, by = c("species_status" = "id")) |>
            
            mutate(
              # Create similar structure to iNaturalist
              time_observed_at = glue("{date} {time}:00") |> as.character(),
              project.id = locationId,
              project.name = names(locationId),
              project.type = "obs",
              user.id = user_detail$id,
              uri_html = glue("<a href='{permalink}'>{permalink}</a>"),
              user.name = ifelse(is.na(user_detail$name) | user_detail$name == '', user_detail$login, user_detail$name),
              location = glue("{map_dbl(point_coordinates, ~ .x[[1]][2])},{map_dbl(point_coordinates, ~ .x[[1]][1])}"),
              uri = permalink,
              scientificName = species_detail_scientific_name,
              vernacularName = species_detail_name,
            ) |>
            # Drop nested columns and convert all to chr for joining with iNat
            select(-c("point_coordinates", "user_detail", "location_detail", "links", "details")) |>
            mutate(across(everything(), as.character))
      
      write_csv(resultsDf, paste0("data/obs_", locationId, ".csv"), quote = "all")
      return(resultsDf)
     },
    error = function(e){
      message('Caught an error!')
      print(e)
      return(NULL)
    },
    warning = function(w){
      message('Caught an warning!')
      print(w)
      return(NULL)
    },
    finally = {
      message('All done, quitting.')
    }
  )
}
```

```{r utils}
#| include: false
#| cache: true

# Filter species level
filterSpeciesLevel <- function(df){
  df %>%  
    filter(
      (project.type == 'inat' & 
       (if (all(has_name(., "taxonRank"))) {
          (taxonRank %in% c('species', 'subspecies'))
        } else {
          FALSE 
        })
      ) |
      (project.type == 'obs' & 
       (if (all(has_name(., "species_detail_type"))) {
          (species_detail_type %in% c('S', 'I'))
        } else {
          FALSE
        })
      )
    )
}

# Filter species level and only research grade or with a valid validation status 
filterSpeciesLevelResearch <- function(df){
  df %>%  
    filter(
      (project.type == 'inat' & 
       (if (all(has_name(., "taxonRank"), has_name(., "quality_grade"))) {
          (taxonRank %in% c('species', 'subspecies')) & quality_grade == 'research'
        } else {
          FALSE 
        })
      ) |
      (project.type == 'obs' & 
       (if (all(has_name(., "species_detail_type"), has_name(., "validation_status"))) {
          (species_detail_type %in% c('S', 'I')) & validation_status %in% c("J", "A", "P")
        } else {
          FALSE
        })
      )
    )
}


```
